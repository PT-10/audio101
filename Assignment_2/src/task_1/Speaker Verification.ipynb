{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/microsoft/UniSpeech.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/fairseq.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:37:09.241215Z",
     "iopub.status.busy": "2025-04-07T02:37:09.240844Z",
     "iopub.status.idle": "2025-04-07T02:37:14.884718Z",
     "shell.execute_reply": "2025-04-07T02:37:14.883820Z",
     "shell.execute_reply.started": "2025-04-07T02:37:09.241189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --force pip==24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:37:15.021332Z",
     "iopub.status.busy": "2025-04-07T02:37:15.021042Z",
     "iopub.status.idle": "2025-04-07T02:38:01.211325Z",
     "shell.execute_reply": "2025-04-07T02:38:01.210168Z",
     "shell.execute_reply.started": "2025-04-07T02:37:15.021308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install s3prl fire omegaconf==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:38:01.219001Z",
     "iopub.status.busy": "2025-04-07T02:38:01.218709Z",
     "iopub.status.idle": "2025-04-07T02:38:01.237187Z",
     "shell.execute_reply": "2025-04-07T02:38:01.236402Z",
     "shell.execute_reply.started": "2025-04-07T02:38:01.218982Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:38:01.238713Z",
     "iopub.status.busy": "2025-04-07T02:38:01.238408Z",
     "iopub.status.idle": "2025-04-07T02:46:44.302076Z",
     "shell.execute_reply": "2025-04-07T02:46:44.301084Z",
     "shell.execute_reply.started": "2025-04-07T02:38:01.238686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --editable ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T15:50:29.715766Z",
     "iopub.status.busy": "2025-04-06T15:50:29.715539Z",
     "iopub.status.idle": "2025-04-06T15:50:38.239528Z",
     "shell.execute_reply": "2025-04-06T15:50:38.238652Z",
     "shell.execute_reply.started": "2025-04-06T15:50:29.715745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-06 15:50:29--  https://mm.kaist.ac.kr/datasets/voxceleb/meta/veri_test2.txt\n",
      "Resolving mm.kaist.ac.kr (mm.kaist.ac.kr)... 143.248.39.47\n",
      "Connecting to mm.kaist.ac.kr (mm.kaist.ac.kr)|143.248.39.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2331882 (2.2M) [text/plain]\n",
      "Saving to: ‘veri_test2.txt’\n",
      "\n",
      "veri_test2.txt      100%[===================>]   2.22M   264KB/s    in 7.4s    \n",
      "\n",
      "2025-04-06 15:50:38 (309 KB/s) - ‘veri_test2.txt’ saved [2331882/2331882]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://mm.kaist.ac.kr/datasets/voxceleb/meta/veri_test2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T15:50:38.242364Z",
     "iopub.status.busy": "2025-04-06T15:50:38.242108Z",
     "iopub.status.idle": "2025-04-06T15:50:38.367853Z",
     "shell.execute_reply": "2025-04-06T15:50:38.367038Z",
     "shell.execute_reply.started": "2025-04-06T15:50:38.242342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mv /kaggle/working/fairseq/veri_test2.txt /kaggle/working/UniSpeech/downstreams/speaker_verification/veri_test2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:36:14.636323Z",
     "iopub.status.busy": "2025-04-07T02:36:14.636051Z",
     "iopub.status.idle": "2025-04-07T02:36:14.998748Z",
     "shell.execute_reply": "2025-04-07T02:36:14.997958Z",
     "shell.execute_reply.started": "2025-04-07T02:36:14.636304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([0, 1, 2], dtype='int64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('/kaggle/working/UniSpeech/downstreams/speaker_verification/veri_test2.txt', sep=\" \", header=None)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:47:58.352769Z",
     "iopub.status.busy": "2025-04-06T16:47:58.352472Z",
     "iopub.status.idle": "2025-04-06T16:47:58.356616Z",
     "shell.execute_reply": "2025-04-06T16:47:58.355728Z",
     "shell.execute_reply.started": "2025-04-06T16:47:58.352747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/kaggle/working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python verification.py --model_name wavlm_base_plus --wav1 /kaggle/input/vox-celeb/vox_celeb/vox1/vox1_test_wav/wav/id10270/x6uYqmx31kE/00001.wav --wav2 /kaggle/input/vox-celeb/vox_celeb/vox1/vox1_test_wav/wav/id10270/8jEAjG6SegY/00008.wav --checkpoint /kaggle/input/wavelm_base_plus/pytorch/default/1/wavlm_base_plus_nofinetune.pth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Verification on VoxCeleb using pretrained WavLM-Base-Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:51:57.004293Z",
     "iopub.status.busy": "2025-04-07T03:51:57.003966Z",
     "iopub.status.idle": "2025-04-07T03:51:57.050735Z",
     "shell.execute_reply": "2025-04-07T03:51:57.049959Z",
     "shell.execute_reply.started": "2025-04-07T03:51:57.004267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# part of the code is borrowed from https://github.com/lawlict/ECAPA-TDNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as trans\n",
    "import soundfile as sf\n",
    "import fire\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "\n",
    "\n",
    "''' Res2Conv1d + BatchNorm1d + ReLU\n",
    "'''\n",
    "\n",
    "\n",
    "class Res2Conv1dReluBn(nn.Module):\n",
    "    '''\n",
    "    in_channels == out_channels == channels\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=True, scale=4):\n",
    "        super().__init__()\n",
    "        assert channels % scale == 0, \"{} % {} != 0\".format(channels, scale)\n",
    "        self.scale = scale\n",
    "        self.width = channels // scale\n",
    "        self.nums = scale if scale == 1 else scale - 1\n",
    "\n",
    "        self.convs = []\n",
    "        self.bns = []\n",
    "        for i in range(self.nums):\n",
    "            self.convs.append(nn.Conv1d(self.width, self.width, kernel_size, stride, padding, dilation, bias=bias))\n",
    "            self.bns.append(nn.BatchNorm1d(self.width))\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        self.bns = nn.ModuleList(self.bns)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        spx = torch.split(x, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            # Order: conv -> relu -> bn\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.bns[i](F.relu(sp))\n",
    "            out.append(sp)\n",
    "        if self.scale != 1:\n",
    "            out.append(spx[self.nums])\n",
    "        out = torch.cat(out, dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "''' Conv1d + BatchNorm1d + ReLU\n",
    "'''\n",
    "\n",
    "\n",
    "class Conv1dReluBn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(F.relu(self.conv(x)))\n",
    "\n",
    "\n",
    "''' The SE connection of 1D case.\n",
    "'''\n",
    "\n",
    "\n",
    "class SE_Connect(nn.Module):\n",
    "    def __init__(self, channels, se_bottleneck_dim=128):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(channels, se_bottleneck_dim)\n",
    "        self.linear2 = nn.Linear(se_bottleneck_dim, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.mean(dim=2)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        out = x * out.unsqueeze(2)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "''' SE-Res2Block of the ECAPA-TDNN architecture.\n",
    "'''\n",
    "\n",
    "\n",
    "class SE_Res2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, scale, se_bottleneck_dim):\n",
    "        super().__init__()\n",
    "        self.Conv1dReluBn1 = Conv1dReluBn(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.Res2Conv1dReluBn = Res2Conv1dReluBn(out_channels, kernel_size, stride, padding, dilation, scale=scale)\n",
    "        self.Conv1dReluBn2 = Conv1dReluBn(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.SE_Connect = SE_Connect(out_channels, se_bottleneck_dim)\n",
    "\n",
    "        self.shortcut = None\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.shortcut:\n",
    "            residual = self.shortcut(x)\n",
    "\n",
    "        x = self.Conv1dReluBn1(x)\n",
    "        x = self.Res2Conv1dReluBn(x)\n",
    "        x = self.Conv1dReluBn2(x)\n",
    "        x = self.SE_Connect(x)\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "''' Attentive weighted mean and standard deviation pooling.\n",
    "'''\n",
    "\n",
    "\n",
    "class AttentiveStatsPool(nn.Module):\n",
    "    def __init__(self, in_dim, attention_channels=128, global_context_att=False):\n",
    "        super().__init__()\n",
    "        self.global_context_att = global_context_att\n",
    "\n",
    "        # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.\n",
    "        if global_context_att:\n",
    "            self.linear1 = nn.Conv1d(in_dim * 3, attention_channels, kernel_size=1)  # equals W and b in the paper\n",
    "        else:\n",
    "            self.linear1 = nn.Conv1d(in_dim, attention_channels, kernel_size=1)  # equals W and b in the paper\n",
    "        self.linear2 = nn.Conv1d(attention_channels, in_dim, kernel_size=1)  # equals V and k in the paper\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.global_context_att:\n",
    "            context_mean = torch.mean(x, dim=-1, keepdim=True).expand_as(x)\n",
    "            context_std = torch.sqrt(torch.var(x, dim=-1, keepdim=True) + 1e-10).expand_as(x)\n",
    "            x_in = torch.cat((x, context_mean, context_std), dim=1)\n",
    "        else:\n",
    "            x_in = x\n",
    "\n",
    "        # DON'T use ReLU here! In experiments, I find ReLU hard to converge.\n",
    "        alpha = torch.tanh(self.linear1(x_in))\n",
    "        # alpha = F.relu(self.linear1(x_in))\n",
    "        alpha = torch.softmax(self.linear2(alpha), dim=2)\n",
    "        mean = torch.sum(alpha * x, dim=2)\n",
    "        residuals = torch.sum(alpha * (x ** 2), dim=2) - mean ** 2\n",
    "        std = torch.sqrt(residuals.clamp(min=1e-9))\n",
    "        return torch.cat([mean, std], dim=1)\n",
    "\n",
    "\n",
    "class ECAPA_TDNN(nn.Module):\n",
    "    def __init__(self, feat_dim=80, channels=512, emb_dim=192, global_context_att=False,\n",
    "                 feat_type='fbank', sr=16000, feature_selection=\"hidden_states\", update_extract=False, config_path=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feat_type = feat_type\n",
    "        self.feature_selection = feature_selection\n",
    "        self.update_extract = update_extract\n",
    "        self.sr = sr\n",
    "\n",
    "        if feat_type == \"fbank\" or feat_type == \"mfcc\":\n",
    "            self.update_extract = False\n",
    "\n",
    "        win_len = int(sr * 0.025)\n",
    "        hop_len = int(sr * 0.01)\n",
    "\n",
    "        if feat_type == 'fbank':\n",
    "            self.feature_extract = trans.MelSpectrogram(sample_rate=sr, n_fft=512, win_length=win_len,\n",
    "                                                        hop_length=hop_len, f_min=0.0, f_max=sr // 2,\n",
    "                                                        pad=0, n_mels=feat_dim)\n",
    "        elif feat_type == 'mfcc':\n",
    "            melkwargs = {\n",
    "                'n_fft': 512,\n",
    "                'win_length': win_len,\n",
    "                'hop_length': hop_len,\n",
    "                'f_min': 0.0,\n",
    "                'f_max': sr // 2,\n",
    "                'pad': 0\n",
    "            }\n",
    "            self.feature_extract = trans.MFCC(sample_rate=sr, n_mfcc=feat_dim, log_mels=False,\n",
    "                                              melkwargs=melkwargs)\n",
    "        else:\n",
    "            if config_path is None:\n",
    "                self.feature_extract = torch.hub.load('s3prl/s3prl', feat_type)\n",
    "            if len(self.feature_extract.model.encoder.layers) == 24 and hasattr(self.feature_extract.model.encoder.layers[23].self_attn, \"fp32_attention\"):\n",
    "                self.feature_extract.model.encoder.layers[23].self_attn.fp32_attention = False\n",
    "            if len(self.feature_extract.model.encoder.layers) == 24 and hasattr(self.feature_extract.model.encoder.layers[11].self_attn, \"fp32_attention\"):\n",
    "                self.feature_extract.model.encoder.layers[11].self_attn.fp32_attention = False\n",
    "\n",
    "            self.feat_num = self.get_feat_num()\n",
    "            self.feature_weight = nn.Parameter(torch.zeros(self.feat_num))\n",
    "\n",
    "        if feat_type != 'fbank' and feat_type != 'mfcc':\n",
    "            freeze_list = ['final_proj', 'label_embs_concat', 'mask_emb', 'project_q', 'quantizer']\n",
    "            for name, param in self.feature_extract.named_parameters():\n",
    "                for freeze_val in freeze_list:\n",
    "                    if freeze_val in name:\n",
    "                        param.requires_grad = False\n",
    "                        break\n",
    "\n",
    "        if not self.update_extract:\n",
    "            for param in self.feature_extract.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.instance_norm = nn.InstanceNorm1d(feat_dim)\n",
    "        # self.channels = [channels] * 4 + [channels * 3]\n",
    "        self.channels = [channels] * 4 + [1536]\n",
    "\n",
    "        self.layer1 = Conv1dReluBn(feat_dim, self.channels[0], kernel_size=5, padding=2)\n",
    "        self.layer2 = SE_Res2Block(self.channels[0], self.channels[1], kernel_size=3, stride=1, padding=2, dilation=2, scale=8, se_bottleneck_dim=128)\n",
    "        self.layer3 = SE_Res2Block(self.channels[1], self.channels[2], kernel_size=3, stride=1, padding=3, dilation=3, scale=8, se_bottleneck_dim=128)\n",
    "        self.layer4 = SE_Res2Block(self.channels[2], self.channels[3], kernel_size=3, stride=1, padding=4, dilation=4, scale=8, se_bottleneck_dim=128)\n",
    "\n",
    "        # self.conv = nn.Conv1d(self.channels[-1], self.channels[-1], kernel_size=1)\n",
    "        cat_channels = channels * 3\n",
    "        self.conv = nn.Conv1d(cat_channels, self.channels[-1], kernel_size=1)\n",
    "        self.pooling = AttentiveStatsPool(self.channels[-1], attention_channels=128, global_context_att=global_context_att)\n",
    "        self.bn = nn.BatchNorm1d(self.channels[-1] * 2)\n",
    "        self.linear = nn.Linear(self.channels[-1] * 2, emb_dim)\n",
    "\n",
    "\n",
    "    def get_feat_num(self):\n",
    "        self.feature_extract.eval()\n",
    "        wav = [torch.randn(self.sr).to(next(self.feature_extract.parameters()).device)]\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extract(wav)\n",
    "        select_feature = features[self.feature_selection]\n",
    "        if isinstance(select_feature, (list, tuple)):\n",
    "            return len(select_feature)\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def get_feat(self, x):\n",
    "        if self.update_extract:\n",
    "            x = self.feature_extract([sample for sample in x])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if self.feat_type == 'fbank' or self.feat_type == 'mfcc':\n",
    "                    x = self.feature_extract(x) + 1e-6  # B x feat_dim x time_len\n",
    "                else:\n",
    "                    x = self.feature_extract([sample for sample in x])\n",
    "\n",
    "        if self.feat_type == 'fbank':\n",
    "            x = x.log()\n",
    "\n",
    "        if self.feat_type != \"fbank\" and self.feat_type != \"mfcc\":\n",
    "            x = x[self.feature_selection]\n",
    "            if isinstance(x, (list, tuple)):\n",
    "                x = torch.stack(x, dim=0)\n",
    "            else:\n",
    "                x = x.unsqueeze(0)\n",
    "            norm_weights = F.softmax(self.feature_weight, dim=-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            x = (norm_weights * x).sum(dim=0)\n",
    "            x = torch.transpose(x, 1, 2) + 1e-6\n",
    "\n",
    "        x = self.instance_norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.get_feat(x)\n",
    "\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out4 = self.layer4(out3)\n",
    "\n",
    "        out = torch.cat([out2, out3, out4], dim=1)\n",
    "        out = F.relu(self.conv(out))\n",
    "        out = self.bn(self.pooling(out))\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def ECAPA_TDNN_SMALL(feat_dim, emb_dim=256, feat_type='fbank', sr=16000, feature_selection=\"hidden_states\", update_extract=False, config_path=None):\n",
    "    return ECAPA_TDNN(feat_dim=feat_dim, channels=512, emb_dim=emb_dim,\n",
    "                      feat_type=feat_type, sr=sr, feature_selection=feature_selection, update_extract=update_extract, config_path=config_path)\n",
    "\n",
    "\n",
    "def init_model(model_name, checkpoint=None):\n",
    "    config_path = None\n",
    "    model = ECAPA_TDNN_SMALL(feat_dim=768, feat_type='wavlm_base_plus', config_path=config_path)\n",
    "    if checkpoint is not None:\n",
    "        state_dict = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(state_dict['model'], strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:52:01.938536Z",
     "iopub.status.busy": "2025-04-07T03:52:01.938244Z",
     "iopub.status.idle": "2025-04-07T03:52:01.945493Z",
     "shell.execute_reply": "2025-04-07T03:52:01.944713Z",
     "shell.execute_reply.started": "2025-04-07T03:52:01.938516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_audio(wav_path, target_sr=16000):\n",
    "    audio, sr = torchaudio.load(wav_path)\n",
    "    if sr != target_sr:\n",
    "        audio = Resample(orig_freq=sr, new_freq=target_sr)(audio)\n",
    "    return audio.squeeze(0)  # shape: (samples,)\n",
    "\n",
    "\n",
    "def load_batch(wav_paths, target_sr=16000, max_len_sec=None):\n",
    "    batch = []\n",
    "    for path in wav_paths:\n",
    "        audio = load_audio(path, target_sr)\n",
    "        if max_len_sec:\n",
    "            max_len = int(target_sr * max_len_sec)\n",
    "            audio = audio[:max_len]  # truncate to fixed length\n",
    "        batch.append(audio)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def verify_batch(model, wav_paths, sr=16000, max_len_sec=None):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load and batch audio\n",
    "    wavs = load_batch(wav_paths, target_sr=sr, max_len_sec=max_len_sec)\n",
    "    wavs = [w.to(device) for w in wavs]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(wavs)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:52:04.870310Z",
     "iopub.status.busy": "2025-04-07T03:52:04.870002Z",
     "iopub.status.idle": "2025-04-07T03:52:05.632322Z",
     "shell.execute_reply": "2025-04-07T03:52:05.631590Z",
     "shell.execute_reply.started": "2025-04-07T03:52:04.870289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "\n",
    "# Optionally clear any tensors stored on GPU\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) and obj.is_cuda:\n",
    "            del obj\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Final memory cleanup\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:52:10.416908Z",
     "iopub.status.busy": "2025-04-07T03:52:10.416570Z",
     "iopub.status.idle": "2025-04-07T03:52:28.522639Z",
     "shell.execute_reply": "2025-04-07T03:52:28.521699Z",
     "shell.execute_reply.started": "2025-04-07T03:52:10.416854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/s3prl/s3prl/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
      "/root/.cache/torch/hub/s3prl_s3prl_main/s3prl/upstream/byol_s/byol_a/common.py:20: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n",
      "Downloading: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_base_plus.pt\n",
      "Destination: /root/.cache/s3prl/download/72cb34edf8a3724c720467cf40b77ad20b1b714b5f694e9db57f521467f9006b.wavlm_base_plus.pt\n",
      "100%|██████████| 360M/360M [00:04<00:00, 85.9MB/s] \n",
      "/root/.cache/torch/hub/s3prl_s3prl_main/s3prl/upstream/wavlm/expert.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "<ipython-input-52-de47239a5d90>:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n"
     ]
    }
   ],
   "source": [
    "model = init_model(model_name=\"wavlm_base_plus\", checkpoint=\"/kaggle/input/wavelm_base_plus/pytorch/default/1/wavlm_base_plus_nofinetune.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T17:42:27.877944Z",
     "iopub.status.busy": "2025-04-06T17:42:27.877652Z",
     "iopub.status.idle": "2025-04-06T17:50:59.004018Z",
     "shell.execute_reply": "2025-04-06T17:50:59.003202Z",
     "shell.execute_reply.started": "2025-04-06T17:42:27.877923Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/s3prl_s3prl_main\n",
      "/usr/local/lib/python3.10/dist-packages/s3prl/upstream/wavlm/expert.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "<ipython-input-40-10f1fcb66021>:302: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
      "100%|██████████| 1177/1177 [08:23<00:00,  2.34it/s]\n",
      "100%|██████████| 37611/37611 [00:04<00:00, 7662.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label     score\n",
      "0      1  0.636597\n",
      "1      0  0.355888\n",
      "2      1  0.564463\n",
      "3      0  0.071327\n",
      "4      1  0.636857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_path = \"/kaggle/input/vox-celeb/vox_celeb/vox1/vox1_test_wav/wav\"\n",
    "\n",
    "# --- Load trial file ---\n",
    "df = pd.read_csv(\"/kaggle/working/UniSpeech/downstreams/speaker_verification/veri_test2.txt\", sep=\" \", header=None)\n",
    "df.columns = ['label', 'wav1', 'wav2']\n",
    "df['wav1_path'] = df['wav1'].apply(lambda x: os.path.join(base_path, x))\n",
    "df['wav2_path'] = df['wav2'].apply(lambda x: os.path.join(base_path, x))\n",
    "\n",
    "# --- Get unique paths ---\n",
    "unique_files = pd.unique(df[['wav1_path', 'wav2_path']].values.ravel())\n",
    "\n",
    "# --- Init model ---\n",
    "model = init_model(model_name=\"wavlm_base_plus\", checkpoint=\"/kaggle/input/wavelm_base_plus/pytorch/default/1/wavlm_base_plus_nofinetune.pth\")\n",
    "model.eval().to(device)\n",
    "\n",
    "# --- Compute embeddings with batching ---\n",
    "embedding_dict = {}\n",
    "batch_size = 4  # Safe choice for limited VRAM\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(unique_files), batch_size)):\n",
    "        batch_paths = unique_files[i:i + batch_size]\n",
    "        \n",
    "        # Load and pad audio to max length in batch\n",
    "        batch_audio = [load_audio(p) for p in batch_paths]\n",
    "        max_len = max(w.shape[0] for w in batch_audio)\n",
    "        batch_padded = [F.pad(w, (0, max_len - w.shape[0])) for w in batch_audio]\n",
    "        batch_tensor = torch.stack(batch_padded).to(device)\n",
    "\n",
    "        # Inference\n",
    "        batch_emb = model(batch_tensor)\n",
    "        batch_emb = F.normalize(batch_emb, p=2, dim=1).cpu()\n",
    "\n",
    "        for path, emb in zip(batch_paths, batch_emb):\n",
    "            embedding_dict[path] = emb\n",
    "\n",
    "        # Free memory\n",
    "        del batch_tensor, batch_emb\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --- Score trials ---\n",
    "scores = []\n",
    "labels = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    emb1 = embedding_dict[row['wav1_path']]\n",
    "    emb2 = embedding_dict[row['wav2_path']]\n",
    "    sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
    "    scores.append(sim)\n",
    "    labels.append(int(row['label']))\n",
    "\n",
    "# --- Output ---\n",
    "df['score'] = scores\n",
    "print(df[['label', 'score']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:29:59.280477Z",
     "iopub.status.busy": "2025-04-06T18:29:59.280127Z",
     "iopub.status.idle": "2025-04-06T18:29:59.825713Z",
     "shell.execute_reply": "2025-04-06T18:29:59.824667Z",
     "shell.execute_reply.started": "2025-04-06T18:29:59.280448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.3610\n",
      "Accuracy at Best Threshold: 91.99%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "output_df = pd.read_csv('/kaggle/working/final_output.csv')\n",
    "\n",
    "best_acc = 0\n",
    "best_thresh = 0\n",
    "thresholds = np.linspace(0, 1, 1001)  # thresholds from 0.000 to 1.000\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds = (output_df['score'] >= thresh).astype(int)\n",
    "    acc = (preds == output_df['label']).mean()\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"Best Threshold: {best_thresh:.4f}\")\n",
    "print(f\"Accuracy at Best Threshold: {best_acc * 100:.2f}%\")\n",
    "\n",
    "# Optionally, apply the best threshold to get predictions\n",
    "output_df['predicted_label'] = (output_df['score'] >= best_thresh).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:30:49.069854Z",
     "iopub.status.busy": "2025-04-06T18:30:49.069546Z",
     "iopub.status.idle": "2025-04-06T18:30:49.341466Z",
     "shell.execute_reply": "2025-04-06T18:30:49.340721Z",
     "shell.execute_reply.started": "2025-04-06T18:30:49.069832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_df.to_csv('/kaggle/working/final_output_with_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:23:45.345670Z",
     "iopub.status.busy": "2025-04-06T20:23:45.345396Z",
     "iopub.status.idle": "2025-04-06T20:23:45.354065Z",
     "shell.execute_reply": "2025-04-06T20:23:45.353303Z",
     "shell.execute_reply.started": "2025-04-06T20:23:45.345639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aac']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "vox2_path = \"/kaggle/input/vox-celeb/vox2_test_aac\"  \n",
    "all_speakers = (os.listdir(vox2_path))\n",
    "print(all_speakers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:55:34.849089Z",
     "iopub.status.busy": "2025-04-07T02:55:34.848749Z",
     "iopub.status.idle": "2025-04-07T02:55:34.857760Z",
     "shell.execute_reply": "2025-04-07T02:55:34.856951Z",
     "shell.execute_reply.started": "2025-04-07T02:55:34.849062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VoxCeleb2AACDataset(Dataset):\n",
    "    def __init__(self, root_dir, id_list, max_len=4, sr=16000):\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.spk_to_id = {spk: i for i, spk in enumerate(sorted(id_list))}\n",
    "        self.max_len = max_len\n",
    "        self.sr = sr\n",
    "\n",
    "        for spk in id_list:\n",
    "            spk_dir = os.path.join(root_dir, spk)\n",
    "            audio_files = glob(f\"{spk_dir}/*/*.m4a\")  # one level deeper\n",
    "            for audio_path in audio_files:\n",
    "                self.samples.append(audio_path)\n",
    "                self.labels.append(self.spk_to_id[spk])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != self.sr:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sr)\n",
    "\n",
    "        # Trim or pad to fixed length (4 seconds)\n",
    "        target_len = self.max_len * self.sr\n",
    "        if waveform.shape[1] > target_len:\n",
    "            start = torch.randint(0, waveform.shape[1] - target_len, (1,))\n",
    "            waveform = waveform[:, start:start + target_len]\n",
    "        else:\n",
    "            waveform = F.pad(waveform, (0, target_len - waveform.shape[1]))\n",
    "\n",
    "        return waveform.squeeze(0), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:55:45.988580Z",
     "iopub.status.busy": "2025-04-07T02:55:45.988298Z",
     "iopub.status.idle": "2025-04-07T02:56:17.795960Z",
     "shell.execute_reply": "2025-04-07T02:56:17.795305Z",
     "shell.execute_reply.started": "2025-04-07T02:55:45.988560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define paths\n",
    "root_dir = \"/kaggle/input/vox-celeb/vox2_test_aac/aac\"\n",
    "all_speakers = sorted(os.listdir(root_dir))\n",
    "\n",
    "# Split: First 100 → train, remaining 18 → test\n",
    "train_speakers = all_speakers[:100]\n",
    "test_speakers = all_speakers[100:]\n",
    "\n",
    "# Dataset init\n",
    "train_dataset = VoxCeleb2AACDataset(root_dir, train_speakers)\n",
    "test_dataset = VoxCeleb2AACDataset(root_dir, test_speakers)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:57:12.583687Z",
     "iopub.status.busy": "2025-04-07T02:57:12.583268Z",
     "iopub.status.idle": "2025-04-07T02:57:12.595854Z",
     "shell.execute_reply": "2025-04-07T02:57:12.594977Z",
     "shell.execute_reply.started": "2025-04-07T02:57:12.583654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, emb_dim, num_classes, scale=30.0, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(num_classes, emb_dim))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        self.s = scale\n",
    "        self.m = margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        W = F.normalize(self.W, dim=1)\n",
    "        x = F.normalize(embeddings, dim=1)\n",
    "        cos_theta = torch.matmul(x, W.t()).clamp(-1, 1)\n",
    "        target_logit = cos_theta[torch.arange(len(labels)), labels]\n",
    "\n",
    "        sin_theta = torch.sqrt(1.0 - target_logit ** 2)\n",
    "        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m\n",
    "\n",
    "        mask = cos_theta > self.th\n",
    "        final_target_logit = torch.where(mask[torch.arange(len(labels)), labels],\n",
    "                                         cos_theta_m,\n",
    "                                         target_logit - self.mm)\n",
    "\n",
    "        cos_theta[torch.arange(len(labels)), labels] = final_target_logit\n",
    "        return F.cross_entropy(self.s * cos_theta, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:59:02.356535Z",
     "iopub.status.busy": "2025-04-07T02:59:02.356168Z",
     "iopub.status.idle": "2025-04-07T02:59:02.370606Z",
     "shell.execute_reply": "2025-04-07T02:59:02.369944Z",
     "shell.execute_reply.started": "2025-04-07T02:59:02.356508Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layers.0.attention\n",
      "encoder.layers.0.attention.k_proj\n",
      "encoder.layers.0.attention.v_proj\n",
      "encoder.layers.0.attention.q_proj\n",
      "encoder.layers.0.attention.out_proj\n",
      "encoder.layers.0.attention.gru_rel_pos_linear\n",
      "encoder.layers.0.attention.rel_attn_embed\n",
      "encoder.layers.1.attention\n",
      "encoder.layers.1.attention.k_proj\n",
      "encoder.layers.1.attention.v_proj\n",
      "encoder.layers.1.attention.q_proj\n",
      "encoder.layers.1.attention.out_proj\n",
      "encoder.layers.1.attention.gru_rel_pos_linear\n",
      "encoder.layers.2.attention\n",
      "encoder.layers.2.attention.k_proj\n",
      "encoder.layers.2.attention.v_proj\n",
      "encoder.layers.2.attention.q_proj\n",
      "encoder.layers.2.attention.out_proj\n",
      "encoder.layers.2.attention.gru_rel_pos_linear\n",
      "encoder.layers.3.attention\n",
      "encoder.layers.3.attention.k_proj\n",
      "encoder.layers.3.attention.v_proj\n",
      "encoder.layers.3.attention.q_proj\n",
      "encoder.layers.3.attention.out_proj\n",
      "encoder.layers.3.attention.gru_rel_pos_linear\n",
      "encoder.layers.4.attention\n",
      "encoder.layers.4.attention.k_proj\n",
      "encoder.layers.4.attention.v_proj\n",
      "encoder.layers.4.attention.q_proj\n",
      "encoder.layers.4.attention.out_proj\n",
      "encoder.layers.4.attention.gru_rel_pos_linear\n",
      "encoder.layers.5.attention\n",
      "encoder.layers.5.attention.k_proj\n",
      "encoder.layers.5.attention.v_proj\n",
      "encoder.layers.5.attention.q_proj\n",
      "encoder.layers.5.attention.out_proj\n",
      "encoder.layers.5.attention.gru_rel_pos_linear\n",
      "encoder.layers.6.attention\n",
      "encoder.layers.6.attention.k_proj\n",
      "encoder.layers.6.attention.v_proj\n",
      "encoder.layers.6.attention.q_proj\n",
      "encoder.layers.6.attention.out_proj\n",
      "encoder.layers.6.attention.gru_rel_pos_linear\n",
      "encoder.layers.7.attention\n",
      "encoder.layers.7.attention.k_proj\n",
      "encoder.layers.7.attention.v_proj\n",
      "encoder.layers.7.attention.q_proj\n",
      "encoder.layers.7.attention.out_proj\n",
      "encoder.layers.7.attention.gru_rel_pos_linear\n",
      "encoder.layers.8.attention\n",
      "encoder.layers.8.attention.k_proj\n",
      "encoder.layers.8.attention.v_proj\n",
      "encoder.layers.8.attention.q_proj\n",
      "encoder.layers.8.attention.out_proj\n",
      "encoder.layers.8.attention.gru_rel_pos_linear\n",
      "encoder.layers.9.attention\n",
      "encoder.layers.9.attention.k_proj\n",
      "encoder.layers.9.attention.v_proj\n",
      "encoder.layers.9.attention.q_proj\n",
      "encoder.layers.9.attention.out_proj\n",
      "encoder.layers.9.attention.gru_rel_pos_linear\n",
      "encoder.layers.10.attention\n",
      "encoder.layers.10.attention.k_proj\n",
      "encoder.layers.10.attention.v_proj\n",
      "encoder.layers.10.attention.q_proj\n",
      "encoder.layers.10.attention.out_proj\n",
      "encoder.layers.10.attention.gru_rel_pos_linear\n",
      "encoder.layers.11.attention\n",
      "encoder.layers.11.attention.k_proj\n",
      "encoder.layers.11.attention.v_proj\n",
      "encoder.layers.11.attention.q_proj\n",
      "encoder.layers.11.attention.out_proj\n",
      "encoder.layers.11.attention.gru_rel_pos_linear\n"
     ]
    }
   ],
   "source": [
    "for name, module in base_model.named_modules():\n",
    "    if \"attention\" in name.lower():\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:59:30.269941Z",
     "iopub.status.busy": "2025-04-07T02:59:30.269583Z",
     "iopub.status.idle": "2025-04-07T02:59:30.944031Z",
     "shell.execute_reply": "2025-04-07T02:59:30.942580Z",
     "shell.execute_reply.started": "2025-04-07T02:59:30.269868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import WavLMModel, WavLMConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load base WavLM\n",
    "model_name = \"microsoft/wavlm-base-plus\"\n",
    "base_model = WavLMModel.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "base_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:59:49.907501Z",
     "iopub.status.busy": "2025-04-07T02:59:49.907046Z",
     "iopub.status.idle": "2025-04-07T02:59:49.915831Z",
     "shell.execute_reply": "2025-04-07T02:59:49.914779Z",
     "shell.execute_reply.started": "2025-04-07T02:59:49.907458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SpeakerModel(nn.Module):\n",
    "    def __init__(self, base_model, emb_dim=192, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.backbone = base_model\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.project = nn.Linear(self.backbone.config.hidden_size, emb_dim)\n",
    "        self.arcface = ArcFaceLoss(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, wavs, labels=None):\n",
    "        outputs = self.backbone(wavs, output_hidden_states=True)\n",
    "        x = outputs.last_hidden_state.transpose(1, 2)  # B x C x T\n",
    "        pooled = self.pool(x).squeeze(-1)  # B x C\n",
    "        emb = self.project(pooled)\n",
    "\n",
    "        if labels is not None:\n",
    "            return self.arcface(emb, labels)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Your SpeakerModel wraps WavLM and a classification head with ArcFace\n",
    "num_classes = len(train_dataset.spk_to_id)\n",
    "model = SpeakerModel(base_model, emb_dim=192, num_classes=num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for wavs, labels in train_loader:\n",
    "        # Ensure mono\n",
    "        if wavs.ndim == 3:\n",
    "            wavs = wavs.mean(dim=1)\n",
    "\n",
    "        # Resample if necessary (optional if already 16kHz)\n",
    "        # wavs = torchaudio.transforms.Resample(orig_freq, 16000)(wavs)\n",
    "\n",
    "        wavs = wavs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(wavs, labels)  # Model handles forward + ArcFace\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7065803,
     "sourceId": 11301534,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 293628,
     "modelInstanceId": 272652,
     "sourceId": 323652,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
