Never be, the last time ever be? You were talking about the learning order, Right., remind me what the topic of the last class was? Triplate loss, you keep? Yes, I factor, we factor and then what is we have? We can learn different kinds of deep learning models to perform. This, we can have different kinds of models to perform this. Now this is, we spoke about CME's network. Now there are 2 things that we can do. We could have, we were talking about CME's network, because we were talking about recognition. Then you can they are doing any kind of recognition or we are doing any kind of verification. What can we do? We can have either triple at loss or we can do CME's network is by And in the CME's. We were using this, we are wait sort of architectures, you have 2 images going through, or 2 data point going through in this case audio signals., we have 2 audio signals going through and that you are learning a shared network in order to tell whether these 2 data points that are going to the belong to the same class or do they belong to different classes. This is 1 of the architectures that was working with LS TF's in which what the ded was. They took this spectrograph. Of data, utterance, now that 2 things that we can do. One to frame wise data from it? Frame by data, what we were doing? Rain during the day but the frame had its own limitations. the other thing that can be done is you can work with utterances. What is an utterance? Atterance is a bit longer version of frame. the frame is time specific. Would be a 25 millisecond frame, it would be a 10 millisecond frame. The more important part is sentence formation. It can still be time specific but it is a bit longer which that it provides your context as and then go into the details of that a bit more. But the difference that happens is when you take utterances you get more context. You get more data to operate with and when you get more data to operate with then you can have better results because.Have more information that you are working with, this is an example of how you can work with LSTM's. in this case what the dedh was they took a spectrogram of data references. They took this batch of features. Right. They learned embedding using LSTM and when they were learning the embedding they were learning embedding for each case. we are doing speaker recognition in this case and in this.Recognition you could with they they try to learn embedding for each class and then have a similarity matrix.? What will that similarity matrix tell you? The similarity matrix was telling me that if you have in pairs of data will it be a positive level, it's a negative level or what is it? what you could do is you could take a CMIs architecture, you have 2 input data points and then for every classCould have emiting slurs and from those emiting you could learn a similarity midwrids which is telling you what is the probability of what are the chances that it belongs to 1 class versus it belongs to other class right positive negative C emiting slurs and with this you can you can learn loss functions in a way that it is using center loss or something of that that it is trying to because it is learning and embedding for every class. when I just learning everybody for every class you can minimize that intra class variation. that is what you will see in the diagram below that the c 1, c 2, c 3 it is trying to bring the the data points of 1 class together and then in 3 is the variations. the center loss and the other losses that we were talking about that day it can work in this fashion. this is 1 of the architectures that you can work with for LSTM's. this is this. This is one of the things, the other thing that you can do is, you can perform L 2 and. Now within speech there 2 things. A lot of previous literature, strategy to literature to exist. A lot of that is text dependent. Because you were given a past phrase. When you have given a past phrase you have to keep on iterating that only in order to be recognized or in order to access something. But today that's not that I mean that is respective in the in some case. If I have a card. In the car if I want to say that let's say I have a car which is being driven by 3 people. The 3 3 drivers who vocationally drive the car. Now when I sit in the car I should be able to tell my car is mg please adjust the car according to my driver preferences. whatever seat you get, the mirror goes where, the side mirror goes where, all of those things should be able to. Based on my voice and I can say M G please set it up according to my choices. And this should not be a specific very specific fast phrase. It should be able to as long as the meaning is same, I can use a combination of words in order to tell this to M G. Or the second driver or the third driver who ever is driving that can't. They should be able to use their own. I was to convey this message, Earlier it was a constraint because we were using, we were just learning with the specific set of words. When order to provide more flexibility, I would to provide this also. in a way do text independent verification. I could book text dependent, I could book text independent verification. What do I need to do? I need to learn now there are 2 things when you are taking a language, When you are taking a test, you need to ensure that it is not just learning how to speak those sentences. What is the difference when a person is speaking those sentences rather it should also be able to learn my teachers, the teachers of the specific audio rather than only the test.We are an example, your heart something called a shortcut learning., there is, there is, sorry, there is something in in today's ML which is called as shortcut learning is you give data, do a model.Today you are not specifying what are the features when you are learning any kind of deep model, you are not specifying what are the features. You are just giving a lot of data and the model is learning on its own.? Now what happens is let's say, I am trying to differentiate between, move and move and stars., night moon and star, 60 seconds. I am trying to differentiate between moon and stars. Now what I do? Let's say you live in the city, you live on campus.? From the same position you want variableity in moon because moon ka size can change The moon will change size tomorrow morning. Let's see if it's full. Same starting position. When you take the data, the positioning of the moon is the same. It's not necessary that the moon's positioning is very not the size is very but the positioning is not changing too much. Correct? But you are collected the data.? What will happen outside LHP or outside the hospital? You guys collected the data. What will happen? Your data will have moon at 1 place, size bearing. There is a placement of stars, different types of stars and you learn that the moon is, how many stars does the moon place itself? You labeled the scene where the moon is the first scene. Is it done Also runs correctly. But let's say tomorrow when you go to some other city where this.Business change, is that Your, your, your in America, my, my in Iraas, where you go to this positioning of the moon and the staffs changed. What I feel officer is the model miss classify star is moon moon in the. Reason? It was not learning the shape. Because the shape of things is left to vary, sometimes getting small, sometimes getting big, sometimes being a star, sometimes being a dwarf. What is more? The position. The position is the thing that is more consistent than the size. Ideally I would to learn a page on size. Besides no variable is not that rather than learning size my model learn the positioning is the important feature and the moment position change. Whatever position comes in the model will serve predicting life. This is a classic example and not a primary catered example. People have found this half aOk, people have observed this happening. We were working, we were working with some sort of health care data, And the data under 1 different kind of compression., COVID data. We got healthiest samples from the US. Because COVID was the time the healthy people were getting funds to get x ray done. It was all positive people who were getting x ray done. all my health data was coming from the US. From some public repositories and my COVID data was India.? The data that was coming from India was coming with certain machines. Because it was collaboration between two hospitals and the data was coming in from those machines and those machines were doing certain kind of compression, certain kind of processing on the data,? US data was doing something else but when model 1 was giving very high Fanta,Vikaas was When we said let's roll it out. let's go. when you are ICMR testing. ICMR said we will do a full project testing this will pad a little test data. Computer computer computer computer computer computer computer that's all that model went to. The trouble was that the data from India was the data for India's data for the for Reason? He learned compression. Rather than learning the variations in COVID versus healthy, it was a lot more easy for the model to learn the variations between you. The way the image was processed. The way the image was processed, the compression of the variations of the compression characteristics of the Indian machines versus the compression characteristics of the compression of the Indian machines. the moment healthy data from the share same machines came. this happens. Now where was I? in order to ensure that your model is learning person, person embedded also. The characteristics of the individual rather than just the characteristic of the tech that has been spoken. Let say eyes being my name is richa, you speak my name is rhaansh. Now tomorrow when I want to pool I want to get access to share his ID and that tomorrow I come and say my name is share and get access to this. Because we are two variations. Next plus the audio. it should not happen that whatever I am trying to do it is it is. It is learning the other thing. If that is simpler for for some reason the other thing is simpler. that is what we try to do is can we do end to end. Expand dependent or text dependent.Returnation of speakers. now if you do end to end and within this end to end what we can do is you are trying to identify what is the speech and you are also trying to identify what is the speaker. Now the intermediate task to be my annual call is speaker recognition but within the intermediate goal before the engine major tool I might have other things I might have a model to recognize language. To them say in English I can talk to MG. MG saidPreference I for you can use different languages as. Let's say the working with 2 languages. you can have N 2 and model which is doing these 2 things. It is doing language prediction plus it is doing. you can have a model which is performing both of these things. Now if you have that kind of a model what happens? You need to ensure that the loss function that you are learning. The loss function, because you asked for the loss function for land protection? Speak in the mission loss function you asked for? Labor is obviously all different, loss functions will also be multiple of them, in that case you can have a multiple of these loss functions combined together in order to be able to predict and and I'll just walk you through 1 of the examples there. you can do this end to end, it's verification and you can do end to end, it's independent also,, then having here as the cosine symbol, you can do this for science andality, I am going to the largest regulation or something. Rather than having to do that, you can do a complete and to end sort of thing where by removing the largest regulation, I can have, I can perform, apply a soft maths and do and to end, speak a verification of the entire classification. What will science and biology score and science and biology score? you are learning a model which is just give you a science and biology score. And which mobility score you are learning on the classes of the which is logistic regress which is giving you the classes out. it's working in 2 stages. we used to do with the teachers, we now select the teacher who inputs the data to classify. rather than doing that, do complete the. coming back to this prem level versus athaaran's level.Verification, this is what I am getting same. What happens at the romantic level? The speed cycles are alive you said are very fine grained love that because of the context right and in that case something that is also done is you do by direction you are going to by direction MSP. we can do when you are doing speaker record issue. Spinker with generation your by direction why not be very very right because aage pichhe ke context of farak nahin pad raha hai itna zyaada jo content hai vah content but when you are doing speech of when you are doingOr any of those sort of things. In that case having a 5 direction networks can significantly help you because you can look at forward you can look at backward or other cases with pichhe vaala update. Right in speech. Level even here for frame level and utterance level the usage changes. When we are doing speech recognition, atarance level might have more benefit because then again you you get to see the entire sentence. Attenance level by director all of it coming together makes a lot more sense. But it's not to say that speaker recognition can't name it. Frame level but anyway. Combination of frames makes an utterance. my my frame could be of millisecond, but my otherwise would be a couple of hundred milliseconds. maybe 50 frames comprise and utterance. Now how will you combine the information when you go from frame to utterance? Any thoughts? How will you combine the information when you go from frame to utterance? Any thoughts? How can you combine frames? Information of frames to go to utterance. 1 thing is simple, earlier that we talked about, we spoke 1 time right, how do we multi model fusion? That is why my basic thing you can do. Attention on mass. that is why I am mad at him. See there different ways of doing it. You have your broad architecture and 200 percent systems. Which is operating it, input data sequence has come, paper teacher extraction has been done in the train and embedding of each frame is done every year. But then when you are going from these embedding to utterance level, what can you do? researches have been proposed.Kinds of methods to combine creation. 1 of those is your tab layer which is marking but temporal average cooling. in temporal average cooling what we can do is whatever features you have learned from the phrase you can combine those embedding in different ways and just a normal combination of those embedding and then we pass through the last function and take in forward.Limitation of this, good part is you are able to combine info from multiple periods, But how? What is the limitation, problem with it? Which layer does the teacher come from?., loss of information, Either you or you would have combined information from different friends to go to an utterance in any case. Now that 2 ways you can do it ya to or here I embedding for combined? Predict each frame with the prediction of the combined course. This did often. In the first 1 I have a joint set of embeddics. The basis of the classification is combined embeddics rather than just embeddics. In the second case, they combine all the embeddics information after predicting them. there will be different kinds of information laws in both the stages. Now in this tab player what is happening is thatEvery frame is getting, every frame is getting equal with it. Correct, but every frame might not be important in what I am saying. Yes, there is a way that I can assign different ways to different.Prains can I put some sort of attention there? Some sort of attention if I am able to incorporated in that cooling, then that attention, modality, whatever attention methodology I am putting up, that can help in further improving the performance and retaining more important features. it is nothing but a sort. Watered combination, if I talk in terms of very my fusion methodologies, it would be a wittered sort of fusion where every same is getting some kind of weightage based on content or uniqueness care., right. that is a difference between your tap player and your self attempt pooling. These are 2 methods that, that were proposed and then there lot of exchange.Of these that can also be proposed. Is that Another thing you can do is in some of them that have all the embedding features coming up, you can do some sort of class serving or mega codebook out of it and then perform other kinds of processing on that. there are other methodologies that have been proposed in this case and the paper that is listed there has details of all of these. Take a look at that we, what are you doing? essentially then we are learning different ways for different frames and and again those waves could be unique to different in the research. For me different references or different frames can I I different ways versus for some other text that I'm speaking that during that text some of the friends can have different., then you get all of these flexibilities that you can import for it while learning. then going from frame to frame. Now these are some of the advancements that have been proposed and speak the recognition with your LSTM's and all, but other thing that we we discussed about close and an open set. All of these variations can be applied to a close set and an open set. Close at a local side with different types of loss functions. The interest in that people observe is you take a loss function. Poverterals something I don't know. The final 5 don't cross centropid and what doesn't? No one is leading the way? Life you discussed in the last class opened up said it only requires when you are doing, when you are doing verification, identification, recognition. what you just need to choose the closest class. It will give you a ranking or salt. You just need to choose the closest class. what you have to deal with is this inter class of the class bring it to the closest class. You might have a little bit of discrimination but when we are doing this open set the other thing that we have to deal with is discrimination.How much is that? Between different classes, how much discrimination is there and how much time has passed. It is not that any kind of shorting I can perform the top shorting will happen. Going from closer to of the set the second thing that is required is applying a threshold and applying a threshold is required because I need to tell you the best him at is the correct match or not or this person doesn'tData se to all right it's not just that there is the output as an ID you would see in the first figure the output is just an ID rather an open set you have 2 teachers you match the output is a minority value and then you have to make a decision on that's a minority value right threshold allow the yah IDs aaie ki nahin or database mein hai right you have to make that additional distinction.To cross end copy basic cross end copy does not work for opency. you have to have some better loss functions for that specific. And it's an interesting and comparatively less exposure research area in case of any kind of technician to open set more. What come look at them. When you come to vision community also known as your 0 short sort of learn. 0 short can't do the class watch it can't say. Now the vision is comparative, a different date for the short. But those those methodologies are difficult to deploy in a problem, life speaker recognition because here you are dealing with classes. You are dealing with individual. how do you assign a new individual if you don't know the argument. What you can do with open set if you don't have the labels over a period of time please.You you get hundred query samples out of those hundred query samples 10 of those youReqired these are these don't exist in my data set, But what you can do over a period of time is cluster these ones. Cluster these ones in the sense that yes, does may say you feel a cluster. Yes, a cluster, the rest of the 5 million. And then you can try to find a mapping if data set permits or if you have new classes available with you.You can try to find a mapping of who is mapping, but just given that it is a data set of very samples. Al limited data set and some query samples at best you what you can do is these don't exist in the data set and then over a period of time perform a mapping of the unknown classes. These are multiple query samples, belonging to the same class. Now in this case what people have done for open seaterlocation.Is that with your cross centrophy or with your central loss., you can do both of those., cross centrophy is there. Tomorrow basic class prediction is there. Along with that, we can also add a central loss or some other loss. The part, some sort of matric rahi hoon. The triplet loss was the talk of the town, Ramesh ji. we can perform some sort of matric la. Now what is matriculating? Have you heard of Matriculated? Medicine is mine,. Can you tell me what it means from the name itself? The name is very representative of what it does. What does a teacher learn? Please. What does metric learning have to do with it? Metric learning. Now what do I mean by metric? Metric, in this case is a loss function, loss function itself, the cross antaropi was your loss function? Right, the cross interview was trying to learn a space where your samples are separable, But not every problem you guys said is as simple as cross interview. in order to learn better features, what we came up with? We came up with something called the striplet laws. What was the striplet laws? Is it positive or negative? That's a good question. And they learn to make triplets and positive and negative. What was that doing? That is a kind of metropolis., the triplet that I lost is in a metropolis of sorts. The LDA that I am in is what they do. What do they do? What is LDA? Disturbance analysis, in the instrument analysis. What does it do? It tries to learn some sort of a projection. It's it's some sort of projection that you're in. Interest in the area within class experience or between class experience zeta was the ratio you would get for your main EMI check. similar call sir, coming back here not as a as between class within class. Various parts in some other terms. Functions, some other laws functions, what you can do when you're going from close set identification to open set identification. You can you can perform this pairwise losses as a contrasted loss. Who can be added? have a minus percent of the plus and plus. the loss function that you are learning is a combination. it is this group of people as as it is learning what your basic laws function. Now metric learning, metric learning. What is the problem with that?, around 20 15 20 16 to 20 20 or, learning was very popular. There are thousands of papers that have come in, matlab learning, hard mining, different sort of loss functions. you did your last function? I have passed a loss to play clause, sphere place, sphere clause, angular clause. Your word of angular clause? Not a movie this time, it's still CNL. I know the laws, the real face, we know it's all your CNL's. what we're trying to do is binary cross interface tomorrow embedding is to operate in the euclidean space. rather than ready operating with just your cross interface or values in the euclidean space.These last functions what they are trying to do is they are trying to in power for it and the logic. science and reality and the local. To science and reality and the local. can you use that method and device laws functions with the concept of in the class and in., combine in those concepts.? Just the centralism, they're just looking at the probability. We were looking at distances right now we're looking at distances between classes or between classes that would be some sort of a combination of software class and center class. The question that is written in the slide. That's a combination of software and center class. I know there is no end to loss functions. People have designed very sophisticated loss functions. what will I do I will I will I will not a few papers that have proposed these last 1. there is an adult case, the laws then the purpose the laws it was specific to faces. But there are very nice geometric interpretation to it and the last 1 can be applied to any of the. Whatever classification you are doing you can be applied to any of those questions. these are some of the last functions that exist for better learning the embedding space. And all of these again these all of these have been proposed with the respect to vision first. All of that was proposed in computer vision, we got the audio from that. And you can always create various ways for for audio or for other other problem states.Yes that that we are interested in right, these are some of the variations that you have for loss functions. Right and then another thing that you can do is, obviously, you can take a form, you can take statistical learning, you can take deep learning, combined with., also random deep features of source. People have done this as. Right, I will not put too much emphasis on this, but these are some of the advancements that have been done with respect to, to see an issue, to be providing models with respect to that. Now you can do and again this is a lot of this is with respect to speaker recognition. But but I said earlier, if you combine multiple tasks speaker recognition. why do I speak of a mission? I am also doing transiting., what about this, Student ABC's for this,,, no kind of television show that you are, youWe update translate to transfer. X comes up in the search, rather than doing that just text can I do? Transcribing? Plus the correct admission. then I can write which are, if you see, person 1 person 2 person 3. What other things can you do? Anything that you would to see. what do we have? We have speaker recognition, we have ASR, emotion. Anything else? Other thing is language, let's say that you are doing transcribing and what am I doing right now? I am using a combination of Indian language. I mean, English. if you do language recognition, Along with the yes sir, if I can do language recognition, won't your translation be better? It's just if I have to transcribe my arts sentences in English? These are the words that are coming in Indian let me translate these is a hindi, let me translate these to English and then do the complete transiting. what do I have? I have an an audio as in. Audio input, I have a network and I have and I have t1 c2 t3 in 4 different tasks. With this side you do these classes 2 1, 2 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, and this network ends, it doesn't end at everyones end, if it is a 20 year deep network, what will it do? After the certain number of players, it will give me some output. You have to have certain number of players, language information, I will share a charge, language information, then it will be correct admission. And all of it will be the 2 ichchha tha. Do it first and dependency is It has to be more than one dependency. Right, the effectiveness of 1 task will be remove the other task. if I remove easily, but easily will perform badly. To the click of my line admission, his performance will be automatically ruined. Some of the things generally there is a dependence. Because tomorrow features us hisaab's class. There are good middle of independent features. But what we are doing is these are all into an learning. It's not individual learning. all of these networks that when we say a multitask learned. It's a single network which is performed all of these multiple tasks together. the emotion is incorporated into the whole thing before that. in that sense emotion should be talked about first, whatever you want to talk about., depending on what are the tasks you are doing for each of these tasks, there will be laws functions. there will be laws functions., how can you help me in any other life or else? What are the do laws function? Training data? Loss function sir? Model, bad. Right you can assign different signings for beds to these plus functions. Let me let me show you 1 thing I think a. Old students, the old PhD student, various people for whom, TV is the thesis.,, I think online students are not able to see this. What I am trying to show you here is, on in students can you see a PDF? But it is browser,It is browsing, it is browsing. You can browse it on your own, you will try to find this if your net is faster. Urgent schedule task, this is the paper, draft schedule task, mitigating negative transfer in multi task learning using dynamic task dropping. what is the idea is we are talking about, we are talking about having multiple loss functions together and then training a single model. What are the other things you can do? Just waiting sometimes it will, sometimes it has been. And this is not a very, you might think that this is a very high statistical network, world class learning and network that I am talking about. But if you come to problems face recognition, it's a very very common thing. For example, if you have face recognition carbon, what are the different tasks that you would do?Guys I will send you the link guys let me come back to slides it's not opening. Slide of mine., slide is visible to you guys online? Yes ma'am. And you're welcome., what I was saying was, give this last chance, Price admission to example I was saying. What would be the different task if you want to create a Priced admission you?Image, let me image of this class. What will be the first step? Detect the face, localization. Localizing the region of interest, that will be the first thing. there are 15 cases in the class. detecting these 15 cases. that is the first step that becomes my even. What did you do before that? Can they provide quality assistance before the image? Is the image good?, let's say my tea is my tea, for the day assessment, My tea is the is localization. What about the tea li? Let's say if you want to do expression recognition, post variation any of those sort of things. Is it not? One question. A question. And might be by who is? You wish? Five cars. And this is very very problem to take for any person recognition system you have to do this.? Maybe there is recognition. What is the output. See this is the another thing I think you have to see. When you are doing audio, my input is not changing. The entire thing is going. I have an audio sample. The whole audio sample was getting longer and longer. In the beginning, love separate into all the frames of the whole year. But in this case what we were doing with these things we are going to just looking even split ki now once you are done localization. Then I don't need to background data. Because better and better and better and better and better. Only to the techie parties going for it. every loss function. How are you learning every loss function? If not for the money, recognition would not be there what is the importance of those loss functions and how do you learn them together? Basic waiting is a simple thing. You can have a combination of things but then you sit down to implement some of these.Multita learning will work on us then how do you want to be able to and there's a lot of new answer again.Learning is an area, a paarandah minute itself. I will share the paper with you, go through the paper. Some people are a few years apart, but very relevant and very interesting observation. Step connections, you draw skip connections. Based on the concept of skip connections, we prefer something called as skip tasks. Drop out and drop out. Drop out. Very counter intuitive. But it helps, it helps train the model better, Similarly, if you have 5 tasks, in the middle what you can do is we drop the few tasks. for some interventions when you are learning for some reports you can drop a few tasks and then come back to learn. Right over over number of reports, we assign different ways. Have some of these regularizer in place. That helps you in.The last something is learning the model better. that is again, both through the paper there are some in some interest in observations there. But but when you have multiple of these how do you go ahead and combined these last functions to create and into and that is something that speaker recognition community is currently working on. Because transcribing and personal recognition and all of that.Really important, Then there are the transformers, the bird models, there are a lot of other models that are being proposed and manned, and the class will try to go into some of the details as to how these models work. But before going into that, the multiple things that we talk about multiple sources of information would be there also. Multiple multiple tasks done here Now other thing that can be done is what if you have multiple sources of information? What do II have a video that has given a say. Friends is still on TV? Right sir, the video has given a say. What they have to do? You have to do this. Try is fine. Who is the man? What are you saying? What do you do? Separate the video and audio streams? Will you make a separate tray for each video? Trial type audio, trial type video, trial type video, time syncOk. No which frame was the person who stole the audio, I will correct the mention and see if I am correct.? The trial's setting has changed. But what if you are you trained a multi model data, you trained a multi model network using models of using data to into modality.Face and audio, But during query time, you did not have access to both the parts. Sorry, you did not have access to both the data. You just had audio, you did not have audio or you just have video, you did not have audio. And something we done about it.Let's say audio have video. Look at the the I come is going off the bottom. you have to in the in the in the right and side with the guys. Can I do this that just based on a single query input. Can I read construct both. Let's order together. If you take a look at the table that is on the left. Is that Audio input is that? Supervised training? Audio output. Audio input. Video output. what have you been doing in the multi model feature? Audio plus video input, audio plus video query, some prediction based on both. When the fusion happens in the frames, right that is my multi model feature. Modality learning. audio plus video. guys testing the training video and the input video. input both and I'm just learning one. let's go. I am I am wasting 1 resource talking about it. There is another concept of learning which is called as a shared representation. What the shared representation learning say is if you have audio and video as your you have learned the features with bothYou and video. Can you perform testing just based on video and just based on. Network one trend different network another trend. If there are two different networks, then there is no problem. if I do that, will he activate a separate network to do the work? But if a single network comes along, a single data call, yes a single data call, a single mode, but different. Then what can you do? Learnings this is something that was performed I think as I used to have in 11 and 20 25 CML ka pick for you right multi model debloody and this is where you proposed by model important and what this said was you have this audio in video you can learn a share in the representation from them they have this often recorded train you can learn a share representation and from this share representation when you defaultEncoding the time to be learned a shareter presentation, but when from this encoding when you are recording, then they are able to record both a audio and video. learn a shareter presentation in a way that you are able to record both. when you perform this kind of learning, when even when you have this is shareter presentation or you have single modularity for data, you can perform the tasks that you are interested in. he, he kind of proposed this kind of a concept of multi model and all has been existing for, for a very long time. In non-deep learning, learning everything else has been existing. There was this new concept that came up in which they proposed this shared representation learning that even in absence of 1, even in the single network train, even in absence of 1 data from 1 domain you can perform whatever task you are interested in perform. that is your multiple sources of information learning and if you take the look at it in your apps. The video and audio. It can help you perform better in tasks transcribing at all. Because you can do lip syncing or you can take data of lip syncing and other sort of things and perform better speaker recognition or transcribing whatever you are interested in. If you have multiple sources of inputs with you. Performing the task or it it helps in better performing the task. For example, if you have this kind of example that I was saying both audio or visual input at the first., multi model networks people have made this multi multi model networks where you can combine data from multiple modalities and again when you combine data from multiple realities how can you combine data?Provide to a single combined network. You can perform this or you can perform what they were doing before. That have the features trained separately, extracted separately, what work together. What is the other thing that you can do? Other thing we did was frame based learning. The process is to take each frame separately and then combine information that you have learnt from different sources of information. all of these. Different sort of things can be performed and people have tried this not only in this vision and all but also in, particularly the audio. Because video and audio are very integrated inputs that you can get. Video is the video with audio all together. this is a this is a very interestingly integrated input that you get. you can train both of these modalities together. We it email and then it's track the information together.Of query, it might be possible that you just got a query sample from telephone. You have a person with confirmation based on face and all your donon. But the query that you got was just an image., we should be able to recognize just based on that image what is the output that we are getting. that is what you have shared ratters and patient in.Right, these are the kinds of things that have been performed and speaker recognition. and speaker recognition care. Just in 1 day's class we'll do. Just the self-supervise models, transform some of these things and we can be landed there. But 1, any questions students online before I go into the other topic?Nothing., if there are no other questions, let me ask you 2 things. There were, I wanted to take an extra class tomorrow, sometimes.