{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11476390,"sourceType":"datasetVersion","datasetId":7178129}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Login HF","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:04:25.626688Z","iopub.execute_input":"2025-05-23T11:04:25.626904Z","iopub.status.idle":"2025-05-23T11:04:26.039091Z","shell.execute_reply.started":"2025-05-23T11:04:25.626878Z","shell.execute_reply":"2025-05-23T11:04:26.038247Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac1a6fd94a34469d8807ecb312218f2d"}},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## Transcription using Hinglish Finetuned Whisper from Huggingface","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\nimport torch\nimport gc\n\ngc.collect()                      # Python garbage collector\ntorch.cuda.empty_cache()         # Clears cached memory from PyTorch\ntorch.cuda.ipc_collect()\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"Oriserve/Whisper-Hindi2Hinglish-Swift\"\n# model_id = \"vasista22/whisper-hindi-large-v2\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n    return_timestamps=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = pipe(\"/kaggle/input/class-recording/class_recording_2025_03_22.mp3\")\n\nwith open(\"/kaggle/working/transcription_output2.txt\", \"w\", encoding=\"utf-8\") as file:\n    file.write(result[\"text\"])\n\nprint(\"Transcription saved to transcription_output2.txt\")\n\n# print(result[\"text\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Translate Hinglish transcription to English ","metadata":{}},{"cell_type":"code","source":"import gc\nimport torch\n\n# Delete the model and tokenizer (if needed)\ndel model\ndel tokenizer\n\n# Empty CUDA cache\ntorch.cuda.empty_cache()\n\n# Run garbage collector\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"rudrashah/RLM-hinglish-translator\")\nmodel = AutoModelForCausalLM.from_pretrained(\"rudrashah/RLM-hinglish-translator\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = model.device\n\n# Template\ntemplate = \"Hinglish:\\n{hi_en}\\n\\nEnglish:\\n\"\n\n# Function to chunk transcription\ndef chunk_transcription(text, tokenizer, max_tokens=30):\n    sentences = sent_tokenize(text)\n    chunks = []\n    current_chunk = \"\"\n    current_token_count = 0\n\n    for sentence in sentences:\n        sentence_tokens = len(tokenizer.encode(sentence, add_special_tokens=False))\n        if current_token_count + sentence_tokens <= max_tokens:\n            current_chunk += \" \" + sentence\n            current_token_count += sentence_tokens\n        else:\n            chunks.append(current_chunk.strip())\n            current_chunk = sentence\n            current_token_count = sentence_tokens\n\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n\n    return chunks\n\ndef extract_english_part(text):\n    \"\"\"\n    Extracts and returns the English part from a string containing 'Hinglish:' and 'English:' sections.\n    \"\"\"\n    if \"English:\" in text:\n        return text.split(\"English:\", 1)[-1].strip()\n    return text.strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Read file and process\nwith open(\"/kaggle/input/class-recording/transcription_output2.txt\", \"r\", encoding=\"utf-8\") as f:\n    transcription_text = f.read()\n\nchunks = chunk_transcription(transcription_text, tokenizer)\n\n# Initialize a list to store the translated chunks\ntranslated_texts = []\n\n# Translate each chunk with tqdm for progress bar\nfor i, chunk in tqdm(enumerate(chunks), total=len(chunks), desc=\"Translating Chunks\"):\n    prompt = template.format(hi_en=chunk, en=\"\")  # your template likely includes \"hi_en: {hi_en} en:\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    output = model.generate(**input_ids, max_new_tokens=350)\n\n    # Decode output and clean special tokens\n    translated = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n\n    # If your model generates the output in the form of \"en: translated text\", clean that\n    if \"en:\" in translated:\n        translated = translated.split(\"en:\", 1)[-1].strip()\n\n    translated = extract_english_part(translated)\n    translated_texts.append(translated)\n\n# Combine all translated chunks\nfinal_translated_output = \"\\n\".join(translated_texts)\n\n# Save the translated text to a new file\nwith open(\"/kaggle/working/translated_lecture_output.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(final_translated_output)\n\nprint(\"Translation completed and saved to '/kaggle/working/translated_lecture_output.txt'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef clean_transcription(text):\n    # List of filler/disfluency words (excluding 'right')\n    filler_words = ['uhm', 'uh', 'ah', 'like', 'you know', 'so', 'well', 'okay', 'ok', 'hmm', 'huh', 'yeah', 'basically', 'actually']\n    \n    # Remove basic filler words (case-insensitive)\n    pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in filler_words) + r')\\b'\n    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n\n    # Special case: remove 'right?' (with optional space before ?)\n    cleaned_text = re.sub(r'\\bright\\s*\\?', '', cleaned_text, flags=re.IGNORECASE)\n    \n    # Remove extra punctuation clutter like multiple question marks or periods\n    cleaned_text = re.sub(r'[?!.]{2,}', '.', cleaned_text)\n    \n    # Replace multiple spaces or awkward newlines with single space\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n    \n    # Clean up extra commas or spaces near punctuation\n    cleaned_text = re.sub(r'\\s+([?.!,])', r'\\1', cleaned_text)\n    \n    return cleaned_text.strip()\n\n# Example usage\nwith open(\"/kaggle/input/class-recording/translated_lecture_output (4).txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\ncleaned = clean_transcription(raw_text)\n\nwith open(\"/kaggle/working/cleaned_lecture_transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(cleaned)\n\nprint(\"Done cleaning the transcript!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\n\n# Delete the model and tokenizer (if needed)\ndel model\ndel tokenizer\n\n# Empty CUDA cache\ntorch.cuda.empty_cache()\n\n# Run garbage collector\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Translate English Transcription to Telugu","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/VarunGumma/IndicTransToolkit\n# !cd IndicTransToolkit\n\n# !pip install --editable . --use-pep517 # required for pip >= 25.0\n\n# # in case it fails, try:\n# # pip install --editable . --use-pep517 --config-settings editable_mode=compat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:16:56.688565Z","iopub.execute_input":"2025-05-23T11:16:56.689103Z","iopub.status.idle":"2025-05-23T11:16:57.911087Z","shell.execute_reply.started":"2025-05-23T11:16:56.689070Z","shell.execute_reply":"2025-05-23T11:16:57.910305Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'IndicTransToolkit'...\nremote: Enumerating objects: 232, done.\u001b[K\nremote: Counting objects: 100% (137/137), done.\u001b[K\nremote: Compressing objects: 100% (78/78), done.\u001b[K\nremote: Total 232 (delta 67), reused 102 (delta 49), pack-reused 95 (from 1)\u001b[K\nReceiving objects: 100% (232/232), 4.38 MiB | 20.78 MiB/s, done.\nResolving deltas: 100% (95/95), done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport warnings\nfrom tqdm import tqdm\nfrom IndicTransToolkit.processor import IndicProcessor\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nwarnings.filterwarnings(\"ignore\")\nmodel_name = \"prajdabre/rotary-indictrans2-en-indic-1B\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nsrc_lang = \"eng_Latn\"\ntgt_lang = \"tel_Telu\"\nip = IndicProcessor(inference=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n).to(device)\n\n\n# Load cleaned input file\nwith open(\"/kaggle/working/cleaned_lecture_transcript.txt\", \"r\", encoding=\"utf-8\") as f:\n    full_text = f.read()\n\ndef chunk_text_simple(text, tokenizer, src_lang=\"eng_Latn\", tgt_lang=\"tel_Telu\", max_tokens=400):\n    tagged_text = f\"{src_lang} {tgt_lang} {text.strip()}\"\n    input_ids = tokenizer(tagged_text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n    chunks = []\n\n    for i in range(0, len(input_ids), max_tokens):\n        chunk_ids = input_ids[i:i + max_tokens]\n        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        chunks.append(chunk_text.strip())\n\n    return chunks\n\n\n# Chunk the input\nchunks = chunk_text_simple(full_text, tokenizer)\n\nprint(f\"Total chunks created: {len(chunks)}\")\n\n# Translate\ntranslated_chunks = []\n\nfor i, chunk in tqdm(enumerate(chunks), total=len(chunks), desc=\"Translating\"):\n    batch = ip.preprocess_batch([chunk], src_lang=src_lang, tgt_lang=tgt_lang)\n    batch = tokenizer(batch, padding=\"longest\", truncation=True, max_length=800, return_tensors=\"pt\").to(device)\n\n    with torch.inference_mode():\n        output = model.generate(\n            **batch,\n            num_beams=10,\n            length_penalty=1.5,\n            repetition_penalty=2.0,\n            num_return_sequences=1,\n            max_new_tokens=800,\n            early_stopping=True\n        )\n\n    decoded = tokenizer.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    postprocessed = ip.postprocess_batch(decoded, lang=tgt_lang)\n    translated_chunks.append(postprocessed[0])\n\n# Save output\nfinal_translation = \"\\n\\n\".join(translated_chunks)\n\nwith open(\"/kaggle/working/translated_lecture_te2.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(final_translation)\n\nprint(\"Translation complete! Saved to '/kaggle/working/translated_lecture_te2.txt'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sarvam Model","metadata":{}},{"cell_type":"code","source":"import traceback\n\ndef read_file(file_path, lang_name):\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            # Read the first 5 lines safely\n            lines = []\n            for _ in range(5):\n                try:\n                    lines.append(next(file))\n                except StopIteration:\n                    break\n\n            print(f\"=== {lang_name} Text (First Few Lines) ===\")\n            print(\"\".join(lines))\n\n            # Read the rest\n            remaining_text = file.read()\n\n            # Combine and count\n            full_doc = \"\".join(lines) + remaining_text\n            total_chars = len(full_doc)\n            print(f\"\\nTotal number of characters in {lang_name} file:\", total_chars)\n\n            return full_doc\n    except FileNotFoundError:\n        print(f\"Error: {file_path} not found.\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred while reading {file_path}: {e}\")\n        return None\n\n\ntelugu_doc = read_file(\"/kaggle/input/class-recording/telugu_translation_wp.txt\", \"Telugu\")\nprint(telugu_doc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk_text(text, max_length=1000):\n    \"\"\"Splits text into chunks of at most max_length characters while preserving word boundaries.\"\"\"\n    if not text:\n        print(\"Error: Provided text is empty.\")\n        return []\n\n    chunks = []\n    \n    while len(text) > max_length:\n        split_index = text.rfind(\" \", 0, max_length)  # Find the last space within limit\n        if split_index == -1:  \n            split_index = max_length  # No space found, force split at max_length\n        \n        chunks.append(text[:split_index].strip())  # Trim spaces before adding\n        text = text[split_index:].lstrip()  # Remove leading spaces for the next chunk\n    \n    if text:\n        chunks.append(text.strip())  # Add the last chunk\n    \n    return chunks\n\n\nif telugu_doc:\n    # Proceed with chunking if the document is valid\n    telugu_text_chunks = chunk_text(telugu_doc)\n    print(f\"Number of chunks created: {len(telugu_text_chunks)}\")\nelse:\n    print(\"Error: Could not read or empty content in the document.\")\n\n# Check the first few chunks for debugging purposes\nif telugu_text_chunks:\n    print(\"First chunk:\")\n    print(telugu_text_chunks[:1])  # Print the first 5 chunks\nelse:\n    print(\"No chunks were created.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:21:37.978715Z","iopub.execute_input":"2025-04-19T16:21:37.979478Z","iopub.status.idle":"2025-04-19T16:21:37.986171Z","shell.execute_reply.started":"2025-04-19T16:21:37.979447Z","shell.execute_reply":"2025-04-19T16:21:37.985444Z"}},"outputs":[{"name":"stdout","text":"Number of chunks created: 44\nFirst chunk:\n['మనం ఇంతకుముందు ఇక్కడ ఉన్నామా? మనం డీప్ లెర్నింగ్ మోడల్స్ గురించి మాట్లాడుకుంటున్నాం కదా? అయితే, గతసారి మనం ఏ విషయం గురించి మాట్లాడుకున్నామో గుర్తుచేసుకోండి? ట్రిప్లెట్ లాస్ చేస్తున్నారు కదా? సరే. అయితే నేను ఒక ఫ్యాక్టర్, ఇంకో ఫ్యాక్టర్ క్యాల్క్యులేట్ చేసి, తర్వాత ఇంకేం ఉంటుంది? దీన్ని పెర్ఫామ్ చేయడానికి మనం రకరకాల డీప్ లెర్నింగ్ మోడల్స్\\u200cని ఉపయోగించవచ్చు కదా? సరే. దీన్ని పెర్ఫామ్ చేయడానికి రకరకాల మోడల్స్ ఉంటాయి. ఇప్పుడు ఇది సీ ఎం ఈస్ నెట్\\u200cవర్క్ గురించి మనం మాట్లాడుకున్నాం కదా? ఇప్పుడు మనం రెండు విషయాలు చెప్పవచ్చు. మనం సీ ఎం ఈస్ నెట్\\u200cవర్క్స్ గురించి మాట్లాడుకుంటున్నాం కదా. ఎందుకంటే, మనం రికగ్నిషన్ గురించి మాట్లాడుకుంటున్నాం? మనం ఏదైనా రికగ్నిషన్ చేస్తున్నప్పుడు, లేదా వెరిఫికేషన్ చేస్తున్నప్పుడు ఏం చేయాలి? మనం ట్రిపుల్ట్ లాస్ చేయవచ్చు లేదా సీ ఎం ఈస్ నెట్\\u200cవర్క్\\u200cలు కూడా చేయవచ్చు. సరేనా? సీ ఎం వి నెట్\\u200cవర్క్స్\\u200cలో మేము ఈ షేర్డ్ వెయిట్ సార్ట్ ఆర్కిటెక్చర్నే యూజ్ చేస్తున్నాం. ఇక్కడ రెండు ఇమేజెస్ లేదా రెండు డేటా పాయింట్స్ ఉంటాయి. ఉదాహరణకి ఆడియో సిగ్నల్స్. కాబట్టి, రెండు ఆడియో సిగ్నల్స్ పాస్']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import requests\n\n# Define API request details\nurl = \"https://api.sarvam.ai/translate\"\nheaders = {\n    \"api-subscription-key\": \"ca70f842-cdc8-4f27-8741-6a1bff54a49b\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Initialize dictionary to store translation results\nchunk_translation_results = {}\n\n# Send requests for each chunk\nfor idx, chunk in enumerate(english_text_chunks):\n    payload = {\n        \"source_language_code\": \"en-IN\",\n        \"target_language_code\": \"te-IN\",\n        \"speaker_gender\": \"Female\",\n        \"mode\": \"classic-colloquial\",\n        \"model\": \"mayura:v1\",\n        \"enable_preprocessing\": False,\n        \"output_script\": \"spoken-form-in-native\",\n        \"input\": chunk\n    }\n\n    try:\n        response = requests.post(url, json=payload, headers=headers)\n        \n        if response.status_code == 200:\n            translated_text = response.json().get(\"translated_text\", \"Translation not available\")\n            chunk_translation_results[idx] = {\n                \"status\": \"success\",\n                \"original\": chunk,\n                \"translated\": translated_text\n            }\n            print(f\"\\n=== Translated Chunk {idx + 1} ===\\n{translated_text}\\n\")\n        else:\n            # If the request fails, store the chunk with an error status\n            chunk_translation_results[idx] = {\n                \"status\": \"failed\",\n                \"original\": chunk,\n                \"translated\": None\n            }\n            print(f\"Error: {response.status_code}, {response.text}\")\n            print(f\"\\n=== Failed Chunk {idx + 1} ===\\n{chunk}\\n\")\n    \n    except Exception as e:\n        # In case of exception (network issue, timeout, etc.), store the error\n        chunk_translation_results[idx] = {\n            \"status\": \"failed\",\n            \"original\": chunk,\n            \"translated\": None,\n            \"error\": str(e)\n        }\n        print(f\"Exception occurred for Chunk {idx + 1}: {e}\")\n        print(f\"\\n=== Failed Chunk {idx + 1} ===\\n{chunk}\\n\")\n\n# Combine all successfully translated chunks\nfinal_translation = \"\\n\".join([result[\"translated\"] for result in chunk_translation_results.values() if result[\"status\"] == \"success\"])\n\n# Save successful translations to a text file\nwith open(\"/kaggle/working/translated_lecture_telugu.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(final_translation)\n\nprint(\"Translation saved to '/kaggle/working/translated_lecture_telugu.txt'\")\n\n# Optionally, you can also print out the chunk translation results dictionary if you want to check the status of each chunk\nprint(\"\\n=== Translation Results (Chunk ID: Status) ===\")\nfor chunk_id, result in chunk_translation_results.items():\n    print(f\"Chunk {chunk_id + 1}: {result['status']}\")\n    if result['status'] == \"failed\":\n        print(f\"Original chunk: {result['original']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Telugu audio in user's voice using reference script","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/ai4bharat/IndicF5.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:05:07.715779Z","iopub.execute_input":"2025-05-23T11:05:07.716347Z","iopub.status.idle":"2025-05-23T11:06:34.270496Z","shell.execute_reply.started":"2025-05-23T11:05:07.716322Z","shell.execute_reply":"2025-05-23T11:06:34.269553Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/ai4bharat/IndicF5.git\n  Cloning https://github.com/ai4bharat/IndicF5.git to /tmp/pip-req-build-2l2lhdo0\n  Running command git clone --filter=blob:none --quiet https://github.com/ai4bharat/IndicF5.git /tmp/pip-req-build-2l2lhdo0\n  Resolved https://github.com/ai4bharat/IndicF5.git to commit b334bb3b1f6a027420cd02c8ffb1fc95b146c6e8\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: accelerate>=0.33.0 in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (1.3.0)\nCollecting cached_path (from f5_tts==0.1.0)\n  Downloading cached_path-1.7.3-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (8.1.8)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (3.5.0)\nCollecting ema_pytorch>=0.5.2 (from f5_tts==0.1.0)\n  Downloading ema_pytorch-0.7.7-py3-none-any.whl.metadata (689 bytes)\nCollecting hydra-core>=1.3.0 (from f5_tts==0.1.0)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (0.42.1)\nRequirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (0.10.2.post1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (3.7.5)\nRequirement already satisfied: numpy<=1.26.4 in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (1.26.4)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (0.25.1)\nCollecting pypinyin (from f5_tts==0.1.0)\n  Downloading pypinyin-0.54.0-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (0.5.2)\nRequirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (0.13.1)\nCollecting tomli (from f5_tts==0.1.0)\n  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (2.5.1+cu124)\nRequirement already satisfied: torchaudio>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (2.5.1+cu124)\nCollecting torchdiffeq (from f5_tts==0.1.0)\n  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (4.67.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (4.51.1)\nCollecting transformers_stream_generator (from f5_tts==0.1.0)\n  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting vocos (from f5_tts==0.1.0)\n  Downloading vocos-0.1.0-py3-none-any.whl.metadata (4.8 kB)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from f5_tts==0.1.0) (0.19.6)\nCollecting x_transformers>=1.31.14 (from f5_tts==0.1.0)\n  Downloading x_transformers-2.3.5-py3-none-any.whl.metadata (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.33.0->f5_tts==0.1.0) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.33.0->f5_tts==0.1.0) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.33.0->f5_tts==0.1.0) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.33.0->f5_tts==0.1.0) (0.30.2)\nRequirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.0->f5_tts==0.1.0) (2.3.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.0->f5_tts==0.1.0) (4.9.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<=1.26.4->f5_tts==0.1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<=1.26.4->f5_tts==0.1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<=1.26.4->f5_tts==0.1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<=1.26.4->f5_tts==0.1.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<=1.26.4->f5_tts==0.1.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<=1.26.4->f5_tts==0.1.0) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->f5_tts==0.1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->f5_tts==0.1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->f5_tts==0.1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->f5_tts==0.1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->f5_tts==0.1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->f5_tts==0.1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->f5_tts==0.1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->f5_tts==0.1.0) (1.3.0)\nRequirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from x_transformers>=1.31.14->f5_tts==0.1.0) (0.8.1)\nCollecting einx>=0.3.0 (from x_transformers>=1.31.14->f5_tts==0.1.0)\n  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\nCollecting loguru (from x_transformers>=1.31.14->f5_tts==0.1.0)\n  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from cached_path->f5_tts==0.1.0) (2.32.3)\nCollecting rich<14.0,>=12.1 (from cached_path->f5_tts==0.1.0)\n  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from cached_path->f5_tts==0.1.0) (1.37.29)\nRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from cached_path->f5_tts==0.1.0) (2.14.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->f5_tts==0.1.0) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->f5_tts==0.1.0) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->f5_tts==0.1.0) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->f5_tts==0.1.0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->f5_tts==0.1.0) (0.70.16)\nCollecting fsspec (from torch>=2.0.0->f5_tts==0.1.0)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->f5_tts==0.1.0) (3.11.16)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (3.0.1)\nRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (1.15.2)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (0.60.0)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (0.5.0.post1)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->f5_tts==0.1.0) (1.1.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->f5_tts==0.1.0) (1.17.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->f5_tts==0.1.0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->f5_tts==0.1.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->f5_tts==0.1.0) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->f5_tts==0.1.0) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->f5_tts==0.1.0) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->f5_tts==0.1.0) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->f5_tts==0.1.0) (2.9.0.post0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->f5_tts==0.1.0) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->f5_tts==0.1.0) (0.21.0)\nCollecting encodec==0.1.1 (from vocos->f5_tts==0.1.0)\n  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->f5_tts==0.1.0) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->f5_tts==0.1.0) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->f5_tts==0.1.0) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->f5_tts==0.1.0) (3.20.3)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->f5_tts==0.1.0) (2.11.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->f5_tts==0.1.0) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->f5_tts==0.1.0) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->f5_tts==0.1.0) (75.1.0)\nRequirement already satisfied: botocore<1.38.0,>=1.37.29 in /usr/local/lib/python3.11/dist-packages (from boto3<2.0,>=1.0->cached_path->f5_tts==0.1.0) (1.37.29)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2.0,>=1.0->cached_path->f5_tts==0.1.0) (1.0.1)\nRequirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2.0,>=1.0->cached_path->f5_tts==0.1.0) (0.11.4)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->f5_tts==0.1.0) (2.22)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb->f5_tts==0.1.0) (1.17.0)\nRequirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->x_transformers>=1.31.14->f5_tts==0.1.0) (2.4.6)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->f5_tts==0.1.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->f5_tts==0.1.0) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->f5_tts==0.1.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->f5_tts==0.1.0) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->f5_tts==0.1.0) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->f5_tts==0.1.0) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->f5_tts==0.1.0) (1.19.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->f5_tts==0.1.0) (4.0.12)\nRequirement already satisfied: google-auth<3.0dev,>=2.23.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (2.27.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (1.34.1)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (2.4.1)\nRequirement already satisfied: google-resumable-media>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (1.6.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->f5_tts==0.1.0) (0.43.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->f5_tts==0.1.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->f5_tts==0.1.0) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->f5_tts==0.1.0) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->cached_path->f5_tts==0.1.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->cached_path->f5_tts==0.1.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->cached_path->f5_tts==0.1.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->cached_path->f5_tts==0.1.0) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.1->cached_path->f5_tts==0.1.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.1->cached_path->f5_tts==0.1.0) (2.19.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa->f5_tts==0.1.0) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->f5_tts==0.1.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=1.26.4->f5_tts==0.1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=1.26.4->f5_tts==0.1.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<=1.26.4->f5_tts==0.1.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<=1.26.4->f5_tts==0.1.0) (2024.2.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->f5_tts==0.1.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->f5_tts==0.1.0) (2025.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->f5_tts==0.1.0) (5.0.2)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (1.67.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (4.9)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<=1.26.4->f5_tts==0.1.0) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.1->cached_path->f5_tts==0.1.0) (0.1.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (0.6.1)\nDownloading ema_pytorch-0.7.7-py3-none-any.whl (9.8 kB)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading x_transformers-2.3.5-py3-none-any.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m126.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cached_path-1.7.3-py3-none-any.whl (36 kB)\nDownloading pypinyin-0.54.0-py2.py3-none-any.whl (837 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.0/837.0 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\nDownloading vocos-0.1.0-py3-none-any.whl (24 kB)\nDownloading einx-0.3.0-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: f5_tts, transformers_stream_generator, encodec\n  Building wheel for f5_tts (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for f5_tts: filename=f5_tts-0.1.0-py3-none-any.whl size=63484 sha256=23bf851dde5b99cea32081f8d48dcca564ae80053984a0cdc99f6b388e9b1af0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-3sjvth4_/wheels/bf/8a/1a/425709aaa848706994d165d99a4fe14147e66ce3c392f0239c\n  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12425 sha256=b19a0369f3b959466c9ff7be37821e0aae930bae40386b9bd557c043bdac8d87\n  Stored in directory: /root/.cache/pip/wheels/23/e8/f0/b3c58c12d1ffe60bcc8c7d121115f26b2c1878653edfca48db\n  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=71f804239f18523870354d910e310668006747549f7662a060810e2761bd276f\n  Stored in directory: /root/.cache/pip/wheels/b4/a4/88/480018a664e58ca7ce6708759193ee51b017b3b72aa3df8a85\nSuccessfully built f5_tts transformers_stream_generator encodec\nInstalling collected packages: tomli, pypinyin, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, loguru, fsspec, rich, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, ema_pytorch, cached_path, encodec, einx, x_transformers, vocos, transformers_stream_generator, torchdiffeq, f5_tts\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: rich\n    Found existing installation: rich 14.0.0\n    Uninstalling rich-14.0.0:\n      Successfully uninstalled rich-14.0.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cached_path-1.7.3 einx-0.3.0 ema_pytorch-0.7.7 encodec-0.1.1 f5_tts-0.1.0 fsspec-2024.12.0 hydra-core-1.3.2 loguru-0.7.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pypinyin-0.54.0 rich-13.9.4 tomli-2.2.1 torchdiffeq-0.2.5 transformers_stream_generator-0.0.5 vocos-0.1.0 x_transformers-2.3.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install transformers==4.49.0 pydub soundfile safetensors huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:31:16.582944Z","iopub.execute_input":"2025-05-23T11:31:16.583618Z","iopub.status.idle":"2025-05-23T11:31:27.000392Z","shell.execute_reply.started":"2025-05-23T11:31:16.583590Z","shell.execute_reply":"2025-05-23T11:31:26.999653Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.49.0\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\nRequirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (0.21.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (4.67.1)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.49.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.49.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.49.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.49.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.49.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.49.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.49.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.49.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.49.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.49.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.49.0) (2024.2.0)\nDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\nSuccessfully installed transformers-4.49.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoModel\nimport numpy as np\nimport soundfile as sf\n\ndevice = \"cuda\"\n\n# Load IndicF5 from Hugging Face\nrepo_id = \"ai4bharat/IndicF5\"\nmodel = AutoModel.from_pretrained(repo_id, trust_remote_code=True).to(device)\n\ntelugu_text_chunks = [\"మనం ఇంతకుముందు ఇక్కడ ఉన్నామా? మనం డీప్ లెర్నింగ్ మోడల్స్ గురించి మాట్లాడుకుంటున్నాం కదా? అయితే, గతసారి మనం ఏ విషయం గురించి మాట్లాడుకున్నామో గుర్తుచేసుకోండి? ట్రిప్లెట్ లాస్ చేస్తున్నారు కదా? సరే. అయితే నేను ఒక ఫ్యాక్టర్, ఇంకో ఫ్యాక్టర్ క్యాల్క్యులేట్ చేసి, తర్వాత ఇంకేం ఉంటుంది? దీన్ని పెర్ఫామ్ చేయడానికి మనం రకరకాల డీప్ లెర్నింగ్ మోడల్స్\\u200cని ఉపయోగించవచ్చు కదా? సరే. దీన్ని పెర్ఫామ్ చేయడానికి రకరకాల మోడల్స్ ఉంటాయి. ఇప్పుడు ఇది సీ ఎం ఈస్ నెట్\\u200cవర్క్ గురించి మనం మాట్లాడుకున్నాం కదా? ఇప్పుడు మనం రెండు విషయాలు చెప్పవచ్చు. మనం సీ ఎం ఈస్ నెట్\\u200cవర్క్స్ గురించి మాట్లాడుకుంటున్నాం కదా. ఎందుకంటే, మనం రికగ్నిషన్ గురించి మాట్లాడుకుంటున్నాం? మనం ఏదైనా రికగ్నిషన్ చేస్తున్నప్పుడు, లేదా వెరిఫికేషన్ చేస్తున్నప్పుడు ఏం చేయాలి? మనం ట్రిపుల్ట్ లాస్ చేయవచ్చు లేదా సీ ఎం ఈస్ నెట్\\u200cవర్క్\\u200cలు కూడా చేయవచ్చు. సరేనా? సీ ఎం వి నెట్\\u200cవర్క్స్\\u200cలో మేము ఈ షేర్డ్ వెయిట్ సార్ట్ ఆర్కిటెక్చర్నే యూజ్ చేస్తున్నాం. ఇక్కడ రెండు ఇమేజెస్ లేదా రెండు డేటా పాయింట్స్ ఉంటాయి. ఉదాహరణకి ఆడియో సిగ్నల్స్. కాబట్టి, రెండు ఆడియో సిగ్నల్స్ పాస్\"]\n\nfor index, input_text in enumerate(telugu_text_chunks):\n    # Now use input_text as input to the model\n    audio = model(\n        input_text,\n        ref_audio_path=\"/kaggle/input/class-recording/ref_output.wav\",\n        ref_text=\"ఈ రెండు డేటా పాయింట్స్ ఒకే క్లాస్‌కి చెందినవా కాదా లేక వేర్వేరు క్లాసెస్‌కి చెందినవా కాదా అని చెప్పడానికి నెట్‌వర్క్‌లో\"\n    )\n    \n    # Normalize and save output\n    if audio.dtype == np.int16:\n        audio = audio.astype(np.float32) / 32768.0\n    sf.write(f\"/kaggle/working/chunk{index}_output.wav\", np.array(audio, dtype=np.float32), samplerate=24000)\n    print(\"Audio saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:32:07.041602Z","iopub.execute_input":"2025-05-23T11:32:07.041885Z","iopub.status.idle":"2025-05-23T11:32:08.299563Z","shell.execute_reply.started":"2025-05-23T11:32:07.041864Z","shell.execute_reply":"2025-05-23T11:32:08.298186Z"}},"outputs":[{"name":"stdout","text":"Download Vocos from huggingface charactr/vocos-mel-24khz\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/4104032563.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load IndicF5 from Hugging Face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ai4bharat/IndicF5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtelugu_text_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"మనం ఇంతకుముందు ఇక్కడ ఉన్నామా? మనం డీప్ లెర్నింగ్ మోడల్స్ గురించి మాట్లాడుకుంటున్నాం కదా? అయితే, గతసారి మనం ఏ విషయం గురించి మాట్లాడుకున్నామో గుర్తుచేసుకోండి? ట్రిప్లెట్ లాస్ చేస్తున్నారు కదా? సరే. అయితే నేను ఒక ఫ్యాక్టర్, ఇంకో ఫ్యాక్టర్ క్యాల్క్యులేట్ చేసి, తర్వాత ఇంకేం ఉంటుంది? దీన్ని పెర్ఫామ్ చేయడానికి మనం రకరకాల డీప్ లెర్నింగ్ మోడల్స్\\u200cని ఉపయోగించవచ్చు కదా? సరే. దీన్ని పెర్ఫామ్ చేయడానికి రకరకాల మోడల్స్ ఉంటాయి. ఇప్పుడు ఇది సీ ఎం ఈస్ నెట్\\u200cవర్క్ గురించి మనం మాట్లాడుకున్నాం కదా? ఇప్పుడు మనం రెండు విషయాలు చెప్పవచ్చు. మనం సీ ఎం ఈస్ నెట్\\u200cవర్క్స్ గురించి మాట్లాడుకుంటున్నాం కదా. ఎందుకంటే, మనం రికగ్నిషన్ గురించి మాట్లాడుకుంటున్నాం? మనం ఏదైనా రికగ్నిషన్ చేస్తున్నప్పుడు, లేదా వెరిఫికేషన్ చేస్తున్నప్పుడు ఏం చేయాలి? మనం ట్రిపుల్ట్ లాస్ చేయవచ్చు లేదా సీ ఎం ఈస్ నెట్\\u200cవర్క్\\u200cలు కూడా చేయవచ్చు. సరేనా? సీ ఎం వి నెట్\\u200cవర్క్స్\\u200cలో మేము ఈ షేర్డ్ వెయిట్ సార్ట్ ఆర్కిటెక్చర్నే యూజ్ చేస్తున్నాం. ఇక్కడ రెండు ఇమేజెస్ లేదా రెండు డేటా పాయింట్స్ ఉంటాయి. ఉదాహరణకి ఆడియో సిగ్నల్స్. కాబట్టి, రెండు ఆడియో సిగ్నల్స్ పాస్\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_members_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_tensor_attributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mfirst_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4340\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtie_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4342\u001b[0;31m         \u001b[0;31m# Set model in evaluation mode to deactivate DropOut modules by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4343\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/IndicF5/b82d286220e3070e171f4ef4b4bd047b9a447c9a/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Load vocoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_vocoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocoder_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"vocos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_local\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Download and load model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/f5_tts/infer/utils_infer.py\u001b[0m in \u001b[0;36mload_vocoder\u001b[0;34m(vocoder_name, is_local, local_path, device, hf_cache_dir)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodec_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mvocoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mvocoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvocoder_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bigvgan\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Cannot copy out of meta tensor; no data!\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                     raise NotImplementedError(\n\u001b[0m\u001b[1;32m   1334\u001b[0m                         \u001b[0;34mf\"{e} Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m                         \u001b[0;34mf\"when moving module from meta to a different device.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."],"ename":"NotImplementedError","evalue":"Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.","output_type":"error"}],"execution_count":6}]}